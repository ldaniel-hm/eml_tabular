{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8lsi5reyDYhQcyNkuBRKn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldaniel-hm/eml_tabular/blob/main/Ejemplo_de_Wrappers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cómo usar los Wrapper Básicos de Gymnasyum.\n",
        "\n",
        "\n",
        "> Luis Daniel Hernández (ldaniel at um.es) - 2025\n"
      ],
      "metadata": {
        "id": "WVJXL8eQHc3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En **Gymnasium**, los **wrappers** son clases que te permiten modificar el comportamiento de un entorno sin alterar directamente el código fuente de este. Existen distintos tipos de wrappers para:\n",
        "\n",
        "1. **Observaciones** (`ObservationWrapper`)  \n",
        "2. **Acciones** (`ActionWrapper`)  \n",
        "3. **Recompensas** (`RewardWrapper`)\n",
        "\n",
        "A continuación se explica cómo puedes crear y usar cada uno de ellos.\n",
        "\n",
        "Como ejemplo final se muestra cómo discretizar tanto las observaiones como las acciones para el entorno Lunar Lander (https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
        "\n",
        "- **Recuerda** que la forma correcta de discritar las observaciones es con mosaicos.\n",
        "\n",
        "- Consulta https://gymnasium.farama.org/api/wrappers/table/ para más wrappers.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "haFcoJ_sHk8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Observation Wrapper**\n",
        "Este tipo de wrapper modifica las observaciones que recibe tu agente. Por ejemplo, imagina que quieres normalizar tus observaciones o aplicar algún procesamiento.\n"
      ],
      "metadata": {
        "id": "xg76mnpnH2BG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5VSEIWiBHVcm"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import Env\n",
        "from gymnasium.core import ObservationWrapper\n",
        "\n",
        "class MyObservationWrapper(ObservationWrapper):\n",
        "    def __init__(self, env: Env):\n",
        "        super().__init__(env)\n",
        "        # Aquí podrías guardar parámetros para la transformación,\n",
        "        # o inicializar variables internas si es necesario.\n",
        "\n",
        "    def observation(self, obs):\n",
        "        # Esta función recibe la observación original y retorna la modificada.\n",
        "\n",
        "        # Ejemplo: Escalar observación dividiendo entre 255 (para imagen)\n",
        "        # return obs / 255.0\n",
        "\n",
        "        # Si no haces nada especial, devuelves la observación original:\n",
        "        return obs\n",
        "\n",
        "# Para usarlo\n",
        "env = MyObservationWrapper(gym.make(\"CartPole-v1\"))\n",
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- El método clave es `observation(self, obs)`, que retorna la nueva observación.\n",
        "- Al usar `MyObservationWrapper`, cada vez que se llame a `env.reset()` o `env.step(...)`, la observación pasará por este método antes de devolverse a tu agente.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DMt4N-7oIEpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Action Wrapper**\n",
        "Modifica las **acciones** que tu agente envía al entorno. Se suele usar para, por ejemplo, **discretizar** o **escalar** acciones cuando tu entorno usa acciones continuas.\n"
      ],
      "metadata": {
        "id": "EStpsD1MIFKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import Env\n",
        "from gymnasium.core import ActionWrapper\n",
        "\n",
        "class MyActionWrapper(ActionWrapper):\n",
        "    def __init__(self, env: Env):\n",
        "        super().__init__(env)\n",
        "        # Puedes guardar parámetros sobre cómo vas a transformar la acción.\n",
        "\n",
        "    def action(self, act):\n",
        "        # Recibe la acción que tu agente emite\n",
        "        # y retorna la acción modificada para el entorno.\n",
        "\n",
        "        # Ejemplo: Rescalar acción a otro rango:\n",
        "        # new_act = act * 2.0\n",
        "        # return new_act\n",
        "\n",
        "        return act\n",
        "\n",
        "    def reverse_action(self, act):\n",
        "        # (Opcional) Define cómo revertir la acción\n",
        "        # si tu agente necesita ver la acción \"original\".\n",
        "        # Esto es útil si necesitas hacer tracking de las acciones\n",
        "        # o en entornos donde uses wrappers anidados.\n",
        "        return act\n",
        "\n",
        "# Para usarlo\n",
        "env = MyActionWrapper(gym.make(\"CartPole-v1\"))\n",
        "obs, info = env.reset()\n",
        "action = 1  # Ejemplo\n",
        "next_obs, reward, done, truncated, info = env.step(action)"
      ],
      "metadata": {
        "id": "M0qsCkRIIDam"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- El método principal es `action(self, act)`, que define cómo se traduce la acción de tu agente a una acción válida para el entorno.\n",
        "- `reverse_action(self, act)` se utiliza para revertir la transformación si fuera necesario.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OlfvEOKHIRYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. **Reward Wrapper**\n",
        "Permite modificar la **recompensa** que retorna el entorno después de cada paso. Por ejemplo, podrías **castigar** más fuertemente un suceso indeseado, o **escalar** la recompensa.\n",
        "\n"
      ],
      "metadata": {
        "id": "p6JYyCszIi3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import Env\n",
        "from gymnasium.core import RewardWrapper\n",
        "\n",
        "class MyRewardWrapper(RewardWrapper):\n",
        "    def __init__(self, env: Env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def reward(self, rew):\n",
        "        # Aquí defines cómo modificar la recompensa original.\n",
        "\n",
        "        # Ejemplo: multiplicar la recompensa por un factor de 0.5\n",
        "        # new_rew = rew * 0.5\n",
        "        # return new_rew\n",
        "\n",
        "        return rew\n",
        "\n",
        "# Para usarlo\n",
        "env = MyRewardWrapper(gym.make(\"CartPole-v1\"))\n",
        "obs, info = env.reset()\n",
        "action = 1  # Ejemplo\n",
        "next_obs, reward, done, truncated, info = env.step(action)\n"
      ],
      "metadata": {
        "id": "9F5Ki9OQIiMs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- El método principal es `reward(self, rew)`, que recibe la recompensa original y devuelve la modificada.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S2WTZszUIkJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Uso de Varios Wrappers a la Vez**\n",
        "Los wrappers se pueden **encadenar**. Por ejemplo, si quieres usar uno de observación y otro de acción al mismo tiempo, puedes simplemente anidar las llamadas:\n",
        "\n",
        "```python\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env = MyActionWrapper(env)\n",
        "env = MyObservationWrapper(env)\n",
        "env = MyRewardWrapper(env)\n",
        "```\n",
        "De esta manera, cada Wrapper se aplicará en el orden en que lo has encadenado.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zz-v1tVnI2fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpuePCNaJ1Lm",
        "outputId": "bc1b1dda-5928-4cff-bdf1-56a51cc3b252"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379450 sha256=99ea38c25a326dd4ba3b00e02b2a8e908cfc9991ee52e24f9fbb8854d68f6500\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import Env\n",
        "from gymnasium.core import ObservationWrapper, ActionWrapper\n",
        "from gymnasium.spaces import MultiDiscrete, Discrete\n",
        "\n",
        "\n",
        "class DiscretizeObservationWrapper(ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Discretiza las observaciones de un entorno que originalmente son continuas.\n",
        "    Para LunarLanderContinuous-v2, la observación tiene 8 variables continuas.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: Env, num_bins=10):\n",
        "        super().__init__(env)\n",
        "        self.num_bins = num_bins\n",
        "\n",
        "        # Obtenemos los límites originales del espacio de observaciones (Box).\n",
        "        # Son arrays de forma (8,). A veces pueden ser muy grandes o incluso inf.\n",
        "        self.obs_low = np.where(np.isfinite(env.observation_space.low),\n",
        "                                env.observation_space.low,\n",
        "                                -1.0)  # Ajuste si hay -inf\n",
        "        self.obs_high = np.where(np.isfinite(env.observation_space.high),\n",
        "                                 env.observation_space.high,\n",
        "                                 1.0)   # Ajuste si hay +inf\n",
        "\n",
        "        # Creamos un espacio MultiDiscrete con 'num_bins' para cada una\n",
        "        # de las 8 dimensiones de observación.\n",
        "        self.observation_space = MultiDiscrete([num_bins]*8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        \"\"\"\n",
        "        Convierte el estado continuo (8,) en un array de 8 índices discretos.\n",
        "        \"\"\"\n",
        "        discrete_obs = []\n",
        "\n",
        "        for i in range(len(obs)):\n",
        "            value = obs[i]\n",
        "            low_i = self.obs_low[i]\n",
        "            high_i = self.obs_high[i]\n",
        "\n",
        "            # En caso de que el rango sea 0 (podría pasar si low==high en algún dim),\n",
        "            # evitamos división por cero\n",
        "            if high_i == low_i:\n",
        "                # Si el rango es cero, devolvemos un solo bin (por ejemplo 0).\n",
        "                discrete_obs.append(0)\n",
        "                continue\n",
        "\n",
        "            # Ajustamos valor a [low_i, high_i]\n",
        "            value = np.clip(value, low_i, high_i)\n",
        "\n",
        "            # Escalamos a [0,1]\n",
        "            scaled_value = (value - low_i) / (high_i - low_i)\n",
        "\n",
        "            # mapeamos a [0, num_bins - 1]\n",
        "            bin_index = int(scaled_value * (self.num_bins - 1))\n",
        "\n",
        "            discrete_obs.append(bin_index)\n",
        "\n",
        "        return np.array(discrete_obs, dtype=np.int64)\n",
        "\n",
        "\n",
        "class DiscretizeActionWrapper(ActionWrapper):\n",
        "    \"\"\"\n",
        "    Discretiza la acción continua del LunarLanderContinuous-v2.\n",
        "    El entorno original tiene acciones en [-1,1] para cada uno\n",
        "    de los 2 motores (main engine y side engine).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: Env, num_bins=5):\n",
        "        super().__init__(env)\n",
        "        self.num_bins = num_bins\n",
        "\n",
        "        # Creamos un espacio MultiDiscrete para las 2 dimensiones de acción,\n",
        "        # con 'num_bins' bins en cada dimensión.\n",
        "        # De este modo, la acción discreta es un vector [a0, a1] con cada a_i en [0..num_bins-1].\n",
        "        self.action_space = MultiDiscrete([num_bins]*2)\n",
        "\n",
        "        # Generamos un array con los valores discretos en [-1, 1].\n",
        "        self.bins = np.linspace(-1, 1, num_bins)\n",
        "\n",
        "    def action(self, act):\n",
        "        \"\"\"\n",
        "        Convierte la acción discreta [idx0, idx1] en una acción continua [val0, val1].\n",
        "        \"\"\"\n",
        "        # La acción 'act' es un array [idx0, idx1].\n",
        "        # Mapeamos idx0 -> valor continuo en la dimensión 0\n",
        "        # Mapeamos idx1 -> valor continuo en la dimensión 1\n",
        "        continuous_action = np.array([\n",
        "            self.bins[act[0]],\n",
        "            self.bins[act[1]]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # El entorno espera una acción de forma (2,) en el rango [-1,1].\n",
        "        return continuous_action\n",
        "\n",
        "    def reverse_action(self, act):\n",
        "        \"\"\"\n",
        "        (Opcional) Método que, dado un valor continuo, retorna el índice discreto.\n",
        "        Aquí no se implementa porque rara vez es necesario.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Creamos el entorno original de LunarLanderContinuous\n",
        "    env = gym.make(\"LunarLanderContinuous-v3\")\n",
        "\n",
        "    # Envolvemos el entorno con nuestros wrappers\n",
        "    env = DiscretizeObservationWrapper(env, num_bins=10)\n",
        "    env = DiscretizeActionWrapper(env, num_bins=5)\n",
        "\n",
        "    # Ejemplo de bucle principal con un agente aleatorio\n",
        "    NUM_EPISODES = 1\n",
        "    for ep in range(NUM_EPISODES):\n",
        "        print(f\"Episodio {ep+1}\")\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            # Acciones aleatorias discretes: [0..num_bins-1] para cada dimensión\n",
        "            discrete_action = env.action_space.sample()\n",
        "            print(f\"Observación {obs}. Acción {discrete_action}\")\n",
        "            obs, reward, done, truncated, info = env.step(discrete_action)\n",
        "            total_reward += reward\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXMu8oCBIjux",
        "outputId": "e3d4fe56-8519-4ec7-a486-9bef735f9b3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodio 1\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [3 3]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [4 3]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [0 0]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [4 0]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [0 0]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [1 0]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [1 0]\n",
            "Observación [4 7 4 4 4 4 0 0]. Acción [0 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 2]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [4 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 2]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [4 4]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [0 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [0 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 2]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [0 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [4 4]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [0 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [0 2]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [3 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [4 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [4 3]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 0]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 2]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [1 1]\n",
            "Observación [4 6 4 4 4 4 0 0]. Acción [2 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [0 0]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [2 0]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [2 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [0 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [3 3]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 1]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 4]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [2 1]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 1]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [3 3]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [4 2]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [3 0]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [2 4]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [0 3]\n",
            "Observación [4 6 4 4 5 4 0 0]. Acción [0 0]\n",
            "Observación [3 6 4 4 5 4 0 0]. Acción [2 0]\n",
            "Observación [3 6 4 4 5 4 0 0]. Acción [3 0]\n",
            "Observación [3 6 3 4 5 4 0 0]. Acción [2 3]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [4 0]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [2 0]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [1 0]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [2 2]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [1 0]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [0 1]\n",
            "Observación [3 5 3 4 5 4 0 0]. Acción [2 2]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [4 3]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [3 3]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [2 2]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [1 3]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [0 0]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [1 0]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [4 2]\n",
            "Observación [3 5 3 3 5 4 0 0]. Acción [3 4]\n",
            "Observación [3 5 3 3 6 4 0 0]. Acción [2 0]\n",
            "Observación [3 5 3 3 6 4 0 0]. Acción [2 4]\n",
            "Observación [3 5 3 3 6 4 0 0]. Acción [0 3]\n",
            "Observación [3 5 3 3 6 4 0 0]. Acción [2 2]\n",
            "Observación [3 5 3 3 6 4 0 0]. Acción [4 4]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [0 1]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [2 0]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [3 4]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [2 4]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [1 3]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [1 0]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [2 1]\n",
            "Observación [3 4 3 3 6 4 0 0]. Acción [3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resumen**\n",
        "1. **`ObservationWrapper`**: se redefine el método `observation` para procesar la **observación**.\n",
        "2. **`ActionWrapper`**: se redefine el método `action` (y opcionalmente `reverse_action`) para procesar la **acción** antes de enviarla al entorno.\n",
        "3. **`RewardWrapper`**: se redefine el método `reward` para modificar la **recompensa**.\n",
        "\n",
        "Estos wrappers te permiten **personalizar** la interacción entre el agente y el entorno sin tocar directamente el código fuente del mismo, lo que facilita la experimentación y la arquitectura limpia en tu proyecto de aprendizaje por refuerzo.\n",
        "\n",
        "Como ejemplo de uso los hemos utilizado para discretizar las observaciones y acciones en un entorno continuo."
      ],
      "metadata": {
        "id": "QAJxq97aI9ip"
      }
    }
  ]
}